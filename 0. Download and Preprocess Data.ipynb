{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/0.%20Download%20and%20Preprocess%20Data.ipynb#Downloading-publication-data-from-Arxiv\" data-toc-modified-id=\"Downloading-publication-data-from-Arxiv-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Downloading publication data from Arxiv</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/0.%20Download%20and%20Preprocess%20Data.ipynb#Downloading-publication-data-from-DBLP\" data-toc-modified-id=\"Downloading-publication-data-from-DBLP-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Downloading publication data from DBLP</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a shell notebook that downloads all of the data from arxiv.org, DBLP, and csrankings.org and calls the Python scripts that do preprocessing. This notebook will take about an hour to run and download several gigabytes of data from the Web.\n",
    "\n",
    "These scripts require `wget`, Python 2, and `nltk`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded files are placed in a subdirectory of the current folder. First you might want to check the output of the below and make sure this is running in a directory that you are happy with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/csutton/research/projects/biblio/arxiv/SuttonGongCSArxiv\n"
     ]
    }
   ],
   "source": [
    "echo $PWD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading publication data from Arxiv\n",
    "\n",
    "First we create the new directories that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTDIR=$PWD/generated/arxiv\n",
    "mkdir -p $OUTDIR\n",
    "mkdir $OUTDIR/raw/\n",
    "mkdir $OUTDIR/json/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scripts that we want to run live in the directory `arxiv/`. They are hardcoded to download files to the directories that we created just above. \n",
    "\n",
    "This script downloads the metadata of the CS and STATS papers from Arxiv using the OAI2 interface as XML. The script makes multiple downloads to handle the paging requirements of OAI. These are all downloaded to `generated/arxiv/raw`. No other processing is performed at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n"
     ]
    }
   ],
   "source": [
    "python2 arxiv/download_arxiv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the next script combines all of the downloaded XML files into a single JSON file, so that they are easier to load later. This also removes some arxiv metadata, like the abstracts, that we don't need in our processing, and extracts the datestamp year into a separate json field, because we especially care about it. Finally, the script tokenizes the title and authors into a bag of words and places the resulting vectors into new fields in the JSON records; we'll use this for record linkage later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 177129 papers from arxiv\n",
      "................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "python2 arxiv/xml2json_arxiv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the date we downloaded everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date > generated/arxiv/arxiv-date-downloaded.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading publication data from DBLP\n",
    "\n",
    "First create the directories we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DBLPDIR=$PWD/generated/dblp\n",
    "mkdir -p $DBLPDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DBLP archive can be downloaded from a single file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-09-20 11:24:13--  http://dblp.uni-trier.de/xml/dblp.xml.gz\n",
      "Resolving dblp.uni-trier.de... 136.199.55.186\n",
      "Connecting to dblp.uni-trier.de|136.199.55.186|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 406271748 (387M) [application/x-gzip]\n",
      "Saving to: ‘/Users/csutton/research/projects/biblio/arxiv/SuttonGongCSArxiv/generated/dblp/dblp.xml.gz’\n",
      "\n",
      "/Users/csutton/rese 100%[===================>] 387.45M  3.12MB/s    in 2m 3s   \n",
      "\n",
      "2017-09-20 11:26:16 (3.16 MB/s) - ‘/Users/csutton/research/projects/biblio/arxiv/SuttonGongCSArxiv/generated/dblp/dblp.xml.gz’ saved [406271748/406271748]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget -O $DBLPDIR/dblp.orig.xml.gz http://dblp.uni-trier.de/xml/dblp.xml.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command replaces XML entities with the actual characters, so that we can just use Unicode with future processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DTD from dblp\n"
     ]
    }
   ],
   "source": [
    "sh dblp/fix-dblp.sh dblp $DBLPDIR/dblp.orig.xml.gz $DBLPDIR/dblp.xml.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scripts in the dblp/ directory are modified versions of the ones here, used to generated csrankings.org https://github.com/emeryberger/CSrankings/tree/gh-pages/util. What this script does is filter all of the publications in DBLP according to the list of venues and years that we are interested in. The lists of venues and years are specified in dblp/setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82427\n",
      "\n",
      "real\t16m39.121s\n",
      "user\t15m39.779s\n",
      "sys\t0m11.387s\n"
     ]
    }
   ],
   "source": [
    "time python2 dblp/regenerate-data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the date we downloaded everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date > generated/dblp/dblp-date-downloaded.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now proceed to notebook 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
